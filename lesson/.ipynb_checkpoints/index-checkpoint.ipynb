{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee53ad89-35b6-4e6a-9438-521964ed132c",
   "metadata": {},
   "source": [
    "# Aws Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89e935-7111-4b1f-8fc8-451cde515379",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998226a-d749-4830-b17a-67561e730e75",
   "metadata": {},
   "source": [
    "In this lesson, we'll walkthrough some code for querying athena.  Please use your skills with reading code when working through this.  So that means placing breakpoints to see what the values of different variables are, and trying to call certain methods yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70af15-361a-4872-af91-486ae7cbc748",
   "metadata": {},
   "source": [
    "### Our starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19d890-afa6-4393-9b5d-afbf9ea2edab",
   "metadata": {},
   "source": [
    "You can see that we have two folders -- our `extract_and_load` folder and our `query_bucket` folder.  And we also have a `src/console.py` file.  \n",
    "\n",
    "The `console.py` file is where we can perform our athena queries.  This is through the `query_results` function.  As you can see, we simply query our input bucket (where our original csv file was stored) as if it were a table.\n",
    "\n",
    "```python\n",
    "query_results(\"SELECT * FROM jigsawtexasquery limit 3\")\n",
    "```\n",
    "\n",
    "You can try this yourself, but you will need to make a couple of tweaks.  \n",
    "\n",
    "* `output_bucket`\n",
    "    * The first is to remember that with athena, when we make a query, our query results get sent to an output bucket.\n",
    "    \n",
    "* `db_name`\n",
    "    * The second is the `db_name`.  This is the name of the [AWS lake formation](https://aws.amazon.com/lake-formation/) that AWS uses to query the s3 bucket.\n",
    "    \n",
    "If you change those variables to point to your `output_bucket` and `db_name`, you should then be able to query your bucket with the input data. \n",
    "\n",
    "Give that a shot.  Change the variable names, and then run `python3 -i console.py` to query the bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f215753-c26b-4d1c-8fd3-cd1eb9f74a96",
   "metadata": {},
   "source": [
    "### How did it work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185a93b-975a-43ff-97dc-c297c6b05cf0",
   "metadata": {},
   "source": [
    "Ok, so now that we queried our bucket, let's take a deeper look at our `query_results` function.  We can see that the function calls two other functions, `query_athena` and `get_query_results`.\n",
    "\n",
    "Understanding why we have two functions depends on understanding how Athena works.  Remember that with Athena, when we query our S3 bucket, the results of that query is stored in a different bucket.  \n",
    "For that reason, we have one function `query_athena` that performs the query, and specifies the `output_bucket`.  And we have another  function `get_query_results` that then retrieves the results from that output bucket.\n",
    "\n",
    "Ok, now let's dig deeper into each of these functions.  You can find them both in `query_bucket/athena_boto.py`.\n",
    "\n",
    "* `query_athena` \n",
    "\n",
    "This function takes the athena `query`, the `db_name`, and the `output_bucket_folder`, where we'll place the results of the query.   Notice that it returns a `response`.  This response **does not** contain the `results` of the query itself.  Rahter it  contains some metadata about the response.  For example, this is an example of what is returned.\n",
    "\n",
    "```python\n",
    "{'QueryExecutionId': '476cf070-6ffd-454a-9ca9-686f23c20b46', 'ResponseMetadata': {'RequestId': '9b0c1adf-f044-492a-9db5-b295cffc1ace', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 23 May 2023 17:24:08 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '59', 'connection': 'keep-alive', 'x-amzn-requestid': '9b0c1adf-f044-492a-9db5-b295cffc1ace'}, 'RetryAttempts': 0}}\n",
    "```\n",
    "\n",
    "We can see that it was successful, and that also there is a `QueryExecutionId`.  \n",
    "\n",
    "That `QueryExecutionId` is the file name of where the results are stored.  For example, we can see that by going to the s3 results bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b737a9-cd82-4f24-982b-579af137d4b3",
   "metadata": {},
   "source": [
    "<img src=\"./s3-results.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fcd5b-9d86-4aa4-bdd8-edc7037aa623",
   "metadata": {},
   "source": [
    "* `get_query_results`\n",
    "Ok, so the main point of `get_query_results` is to just read down our data from the bucket.  However, you will see a `while True` with a `try` `except` block.  We'll see that in that block we repeatedly catch an exception where the query is not yet finished.  So that's the purpose of our `try` `except` block -- repeatedly see if our query has completed.  \n",
    "\n",
    "Then when it has, we call the `read_from_bucket` function at the end.\n",
    "\n",
    "* `read_from_bucket`\n",
    "\n",
    "The `read_from_bucket` method uses our `s3.get_object` method, providing the bucket name, and then the path to the file as the key.  Because the data that comes back is bytes -- which apparently pandas cannot translate to a dataframe -- we convert this to a string with the line `csv = BytesIO(data)`, and then pass this data to our `pd.read_csv` method to return a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42122a79-1a6e-4dd7-ae35-b5333070c834",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78f5c1-bbbe-4455-99fa-dd4f7c8c223d",
   "metadata": {},
   "source": [
    "Ok, so we just went through querying athena.  But remember that this is part of a broader data pipeline, which you can see in the `extract_load` folder.  If you look at the `extract_load/upload_console.py` file, you can see how we got that data in our bucket in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3627dc2-b1ba-4feb-8594-62c466e7b60b",
   "metadata": {},
   "source": [
    "```python\n",
    "# upload_console.py\n",
    "restaurant_name = 'HONDURAS MAYA CAFE & BAR LLC'\n",
    "df, file_name = request_and_download_locally(restaurant_name)\n",
    "uploaded_text = upload_and_read(file_name, query_bucket_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55aa17-2906-4f2a-95c7-5ae9098f7e4c",
   "metadata": {},
   "source": [
    "We did so by first making a request to the api, downloading the results in a csv file, and then uploading those results into our s3 bucket.  From there, we created a datalake that had access to this s3 bucket.  And we used Athena to query the bucket, storing the query results in a separate bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2475dc1-50fe-4133-81af-5176d3a4bd2a",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[AWS Athena](https://www.sqlshack.com/an-introduction-to-aws-athena/)\n",
    "\n",
    "[Athena pros and cons](https://towardsaws.com/aws-athena-why-is-it-different-than-mysql-93d55fd4a757)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee326931-1536-4fad-9dc2-23a50ba90834",
   "metadata": {},
   "source": [
    "[AWS permissions](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e068d53-e240-4220-b10e-d5a2cc2649a2",
   "metadata": {},
   "source": [
    "[S3 permissions](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674864e-b68f-47d1-9bf3-f207b76eb9db",
   "metadata": {},
   "source": [
    "[boto bucket policy](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-bucket-policies.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
